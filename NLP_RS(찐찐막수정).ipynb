{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "X96uBlHqrqmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "GDoVnwVsDiaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW5qskZnDNch"
      },
      "outputs": [],
      "source": [
        "#깃허브에서 KoBERT 파일 로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 필요 라이브러리"
      ],
      "metadata": {
        "id": "mTMvxmheru_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "_nGw9YWuDhBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "id": "XYqePqQ7D43S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669347f9-6459-4514-f728-832d0c4ce614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터셋 불러오기"
      ],
      "metadata": {
        "id": "-h22_s49r155"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "emotion_data = pd.read_excel('/content/drive/MyDrive/CapstonDesign/한국어 감정대화데이터셋.xlsx')"
      ],
      "metadata": {
        "id": "0IZiW9CeD7_v"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리"
      ],
      "metadata": {
        "id": "6UCh_b2Lr4A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_data.sample(n=10)\n",
        "\n",
        "emotion_data.loc[(emotion_data['Emotion'] == \"공포\"), 'Emotion'] = 0  #공포 => 0\n",
        "emotion_data.loc[(emotion_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  #놀람 => 1\n",
        "emotion_data.loc[(emotion_data['Emotion'] == \"분노\"), 'Emotion'] = 2  #분노 => 2\n",
        "emotion_data.loc[(emotion_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  #슬픔 => 3\n",
        "emotion_data.loc[(emotion_data['Emotion'] == \"행복\"), 'Emotion'] = 4  #행복 => 4\n",
        "\n",
        "data_list = []\n",
        "for q, label in zip(emotion_data['Sentence'], emotion_data['Emotion'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    data_list.append(data)\n",
        "\n",
        "print(data_list[0])\n",
        "print(data_list[6000])\n",
        "print(data_list[12000])\n",
        "print(data_list[18000])\n",
        "print(data_list[24000])\n",
        "print(data_list[-1])"
      ],
      "metadata": {
        "id": "IxEeoNJ4D9bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f889e3-04a0-4dcc-938e-41964b22409e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['언니 동생으로 부르는게 맞는 일인가요..??', '0']\n",
            "['기술적으로도 아직도 해체해서 다시 완벽히 돌려놓는게 어려운데 해체를한다고?', '1']\n",
            "['당연히 그렇게 해야지 우리나라도 판매를 중단하라', '2']\n",
            "['그거들은 뒤부터 미치겠어요...', '3']\n",
            "['대박한 앨범인 것 같아요ㅠㅠ', '4']\n",
            "['유재석 오라버니 해피투게더 봤어요', '4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train data & Test data"
      ],
      "metadata": {
        "id": "FRHwA2yrsB7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test 데이터로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ],
      "metadata": {
        "id": "mQjU6ZAGD-sg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2eda737-5825-44ea-c6d5-9d58e1d29cd6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21251\n",
            "7084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBERT 입력 데이터로 만들기"
      ],
      "metadata": {
        "id": "QeCF8frtsNQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "\n",
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n",
        "\n",
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
        "\n",
        "# 토큰화와 패딩이 잘 이루어져있는지 확인\n",
        "data_train[0]\n",
        "\n",
        "# torch 형식의 dataset\n",
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "FD11di4TD_0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d534e823-7983-41df-84b5-b0e7e9b2758f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KoBERT 학습모델 만들기"
      ],
      "metadata": {
        "id": "H14uD2Znsvmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=5,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "R2p8EwS9EENb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18e7771-5f44-466e-a5fa-795cf06a6e51"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7feefd17a0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 저장 및 불러오기"
      ],
      "metadata": {
        "id": "yCq3_P6KptkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model = torch.load('/content/drive/MyDrive/CapstonDesign/model.pt')"
      ],
      "metadata": {
        "id": "021TFYjbqFt2"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 콘텐츠 기반 필터링(ALBUM 150)\n",
        "- 컬럼 정보\n",
        "    - Ranking : 음악 랭킹\n",
        "    - Music Name : 음악 명\n",
        "    - Artist Name : 작곡가\n",
        "    - Genres : 장르\n",
        "    - Average Rating : 평균 순위\n",
        "    - Number of Ratings : 조회수\n",
        "    - Number of Reviews : 음악 리뷰수"
      ],
      "metadata": {
        "id": "uC4QzCHM1qsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fear = \"Sweet but Psycho\"\n",
        "surprise = \"Moning Mood\"\n",
        "anger = \"Centuries\"\n",
        "sadness = \"Alone\"\n",
        "happy = \"Power Up\""
      ],
      "metadata": {
        "id": "0uf3RETEDTPI"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### - 데이터 읽기\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "music = pd.read_excel('/content/drive/MyDrive/CapstonDesign/MusicDataset/Album_1000_dataset.xlsx')\n",
        "music_df = music[['Ranking', 'Music Name', 'Artist Name', 'Genres', 'Average Rating', 'Number of Ratings', 'Number of Reviews']]\n",
        "music_df.head(2)\n",
        "\n",
        "# genres_literal CountVectorize 수행\n",
        "count_vect = CountVectorizer(min_df=0, ngram_range=(1, 2))\n",
        "genre_mat = count_vect.fit_transform(music_df['Genres'])\n",
        "\n",
        "genre_sim = cosine_similarity(genre_mat, genre_mat)\n",
        "\n",
        "# 내림차순 정렬을 위해 -1 옵션을 추가로 준다\n",
        "genre_sim_sorted_ind = genre_sim.argsort()[:, ::-1]\n",
        "\n",
        "\n",
        "# 추천 영화 DataFrame 반환 함수\n",
        "def find_sim_music(df, sorted_ind, title_name, top_n=30):\n",
        "    # 특정 영화 정보 뽑아냄\n",
        "    title_music = df[df['Music Name']==title_name]\n",
        "    title_index = title_music.index.values\n",
        "    similar_indexes = sorted_ind[title_index, :(top_n)].reshape(-1)\n",
        "\n",
        "    return df.iloc[similar_indexes]\n",
        "\n",
        "\n",
        "def fear_music_recommend():\n",
        "    similar_music = find_sim_music(music_df, genre_sim_sorted_ind, fear, 30)\n",
        "    return similar_music.sample(n=5)[['Music Name', 'Artist Name']]\n",
        "    \n",
        "\n",
        "\n",
        "def surprise_music_recommend():\n",
        "    similar_music = find_sim_music(music_df, genre_sim_sorted_ind, surprise, 30)\n",
        "    return similar_music.sample(n=5)[['Music Name', 'Artist Name']]\n",
        "\n",
        "\n",
        "def anger_music_recommend():\n",
        "    similar_music = find_sim_music(music_df, genre_sim_sorted_ind, anger, 30)\n",
        "    return similar_music.sample(n=5)[['Music Name', 'Artist Name']]\n",
        "\n",
        "\n",
        "def sadness_music_recommend():\n",
        "    similar_music = find_sim_music(music_df, genre_sim_sorted_ind, sadness, 30)\n",
        "    return similar_music.sample(n=5)[['Music Name', 'Artist Name']]\n",
        "\n",
        "\n",
        "def happy_music_recommend():\n",
        "    similar_music = find_sim_music(music_df, genre_sim_sorted_ind, happy, 30)\n",
        "    return similar_music.sample(n=5)[['Music Name', 'Artist Name']]"
      ],
      "metadata": {
        "id": "d0C9GLh2BMlz"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새로운 문장 테스트"
      ],
      "metadata": {
        "id": "5aTO6xSjs698"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"공포가\")\n",
        "                print(\">> 오늘의 문장에서 \" + test_eval[0] + \" 느껴집니다.\", \"\\n\")\n",
        "                print(fear_music_recommend())\n",
        "\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"놀람이\")\n",
        "                print(\">> 오늘의 문장에서 \" + test_eval[0] + \" 느껴집니다.\", \"\\n\")\n",
        "                print(surprise_music_recommend())\n",
        "                \n",
        "            elif np.argmax(logits) == 2:\n",
        "                test_eval.append(\"분노가\")\n",
        "                print(\">> 오늘의 문장에서 \" + test_eval[0] + \" 느껴집니다.\", \"\\n\")\n",
        "                print(anger_music_recommend())\n",
        "\n",
        "            elif np.argmax(logits) == 3:\n",
        "                test_eval.append(\"슬픔이\")\n",
        "                print(\">> 오늘의 문장에서 \" + test_eval[0] + \" 느껴집니다.\", \"\\n\")\n",
        "                print(sadness_music_recommend())\n",
        "\n",
        "            elif np.argmax(logits) == 4:\n",
        "                test_eval.append(\"행복이\")\n",
        "                print(\">> 오늘의 문장에서 \" + test_eval[0] + \" 느껴집니다.\", \"\\n\")\n",
        "                print(happy_music_recommend())           "
      ],
      "metadata": {
        "id": "JRe96KOyEMIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e2e0d0-50f0-405c-99de-112abe3430cd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "end = 1\n",
        "while end == 1:\n",
        "    sentence = input(\"감정분석을 위해 문장을 입력해주세요 : \")\n",
        "    if sentence == '0':\n",
        "        break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "N-WF1nBH1EiJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc6d2a0-9cf2-4dc6-a2dc-660f5343c359"
      },
      "execution_count": 95,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "감정분석을 위해 문장을 입력해주세요 : 기분 좋아\n",
            ">> 오늘의 문장에서 행복이 느껴집니다. \n",
            "\n",
            "     Music Name        Artist Name\n",
            "138  Next Level                에스파\n",
            "147       붉은 노을            BIGBANG\n",
            "125  내가 제일 잘 나가               2NE1\n",
            "129         빠빠빠  크레용 팝(Crayon Pop)\n",
            "133  After LIKE                아이브\n",
            "\n",
            "\n",
            "감정분석을 위해 문장을 입력해주세요 : 자연어처리 성공\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> 오늘의 문장에서 놀람이 느껴집니다. \n",
            "\n",
            "               Music Name Artist Name\n",
            "46                    밤편지         아이유\n",
            "39  Four Seasons 'Winter'   Piazzolla\n",
            "37     Piano Sonata No.21      Mozart\n",
            "47                     안녕          폴킴\n",
            "38              Fur Elise   Beethoven\n",
            "\n",
            "\n",
            "감정분석을 위해 문장을 입력해주세요 : 추천시스템도 끝\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> 오늘의 문장에서 놀람이 느껴집니다. \n",
            "\n",
            "                Music Name Artist Name\n",
            "34  Eine Kleine Nachtmusik      Mozart\n",
            "59                  카페에 앉아     원 모어 찬스\n",
            "53                      꽃길         김세정\n",
            "54                  그대라는 시          태연\n",
            "30             Moning Mood       Grieg\n",
            "\n",
            "\n",
            "감정분석을 위해 문장을 입력해주세요 : 다했어!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> 오늘의 문장에서 행복이 느껴집니다. \n",
            "\n",
            "    Music Name   Artist Name\n",
            "143     Psycho          레드벨벳\n",
            "135  LOVE DIVE           아이브\n",
            "147      붉은 노을       BIGBANG\n",
            "128       나팔바지       싸이(PSY)\n",
            "144   긴 생머리 그녀  틴탑 (TEENTOP)\n",
            "\n",
            "\n",
            "감정분석을 위해 문장을 입력해주세요 : 내일 발표만 잘하자...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> 오늘의 문장에서 공포가 느껴집니다. \n",
            "\n",
            "          Music Name          Artist Name\n",
            "29    Electric Shock                 f(x)\n",
            "0   Sweet but Psycho              Ava Max\n",
            "20          Lollipop  빅뱅 (Bigbang) ＆ 2NE1\n",
            "23                TT                TWICE\n",
            "2           Rockabye         Clean Bandit\n",
            "\n",
            "\n",
            "감정분석을 위해 문장을 입력해주세요 : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 참고문헌"
      ],
      "metadata": {
        "id": "vZLEkutCqA_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://velog.io/@seolini43/KOBERT%EB%A1%9C-%EB%8B%A4%EC%A4%91-%EB%B6%84%EB%A5%98-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0-%ED%8C%8C%EC%9D%B4%EC%8D%ACColab \n",
        "- https://hoit1302.tistory.com/159\n",
        "- https://hipster4020.tistory.com/109\n",
        "- 김경재, 「BERT 기반 감성분석을 이용한 추천시스템」, 동국대학교 일반논문, 2021.03\n",
        "- https://sig413.tistory.com/5\n"
      ],
      "metadata": {
        "id": "_2YC57Avnz-S"
      }
    }
  ]
}